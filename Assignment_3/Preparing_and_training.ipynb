{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preparing_and_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df3GztInSLHb"
      },
      "source": [
        ">get access to google drive to store files there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUX0kZhbtNb5",
        "outputId": "72d2f38d-8eb4-46b1-9d42-e4ea74d84f9f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOrYJksfS4dB"
      },
      "source": [
        "> Impots all the requirments in other cells\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtWBFbFwBdUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "544fca5e-abf4-4f0d-d8b1-0c8e5d9165e1"
      },
      "source": [
        "import pandas as pd\n",
        "! pip install pickle5\n",
        "import pickle5 as pickle\n",
        "import itertools\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "from re import sub, findall"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pickle5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 24.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 13.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp36-cp36m-linux_x86_64.whl size=218620 sha256=f010c3735f9b0e2338915d42144ab337e4088324a22c68632a3b34acc378d9c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZh5s7dOBpM_"
      },
      "source": [
        ">constants values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdS-H_aZBZFe"
      },
      "source": [
        "diacritics = '‌|‮|‫|‬|⁩|⁯|⁭|­|‎|‍|‏|‪|⁠|⁧|्ۭۤिۗੱٗਿٰٞۚٓ°°·▫¸：°:⋅`▪̋ˆ¸ˈʿًٌٍَُِّءۙ˝ْٖۢٔۖۡ⋅'\n",
        "punctuations = '[!\"#$%&\\'()٭*+-/;<=>@[\\\\]_{|}~£©«®±»†…•←−≠▌▐▬◊●◾♥♦✅✔➡《》口 员⁧⁩子宇平日本椅火‌窗箭航面곤쌍절￼�🔴‌🔶🔸🔹🖋🥇🥈🥉⁦‹›⁄É×àáãäçèéê→­ë„ì﴿™⌘▪`◄被♫ïñšн^○æ¼ř´ˈ²ā‐стćЭӨ§л✓˝ирБ½↓Чбуоɔøɣагəâдэ者ēÄγМ α爆﴾❤⁣ī″Á️óöсˉйí¤↑ɪ÷ûüý\\\\\\ĀčěğİıłŞŠḤṣ–—Ẓū،‘’۞“”٪٫٬]'\n",
        "\n",
        "zero_space = u'\\u200C|\\u200B|\\uFEFF|\\u180E'\n",
        "\n",
        "given_doc_root_path = 'drive/MyDrive/Assignment_03/data/News/'\n",
        "document_root_path = 'drive/MyDrive/Assignment_03/documents/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bItEWuWZB3Bo"
      },
      "source": [
        ">language toolkits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5afm-r4B7qF"
      },
      "source": [
        "from re import sub, findall\n",
        "\n",
        "def normalizer(news):\n",
        "    # remove english letters, special characters and punctuations\n",
        "    news = sub('[A-Za-z]', ' ', news)\n",
        "    # map all the used character to an uniform one given in simple_mapping in order to homogenize words\n",
        "    news = sub_alphabets(news)\n",
        "    for ch in punctuations:\n",
        "        news = news.replace(ch, ' ')\n",
        "    for ch in diacritics:\n",
        "        news = news.replace(ch, '')\n",
        "\n",
        "    # replace all the numbers with 'N'\n",
        "    dg = findall(r\"\\d+\", news)\n",
        "    dg = sorted(dg, key=len, reverse=True)\n",
        "\n",
        "    for num in dg:\n",
        "        news = news.replace(str(num), ' N ')\n",
        "    news = news.replace('.', '. ').replace('?', ' ؟').replace('؟', ' ؟').replace('؛', ' ؛')\n",
        "    return ' '.join(news.split())\n",
        "\n",
        "\n",
        "# #     based on parsivar's Normalizer().sub_alphabets() function and changed it in the needed way\n",
        "def sub_alphabets(doc_string):\n",
        "    # try:\n",
        "    #     doc_string = doc_string.decode('utf-8')\n",
        "    # except UnicodeEncodeError:\n",
        "    #     pass\n",
        "\n",
        "    #  #    edited parsivar's function based on whats interested in this assignment\n",
        "    #  #    example: turn 'آسمان' into 'اسمان' as they have similar meaning and we do not consider only the correct written form\n",
        "    a0 = 'ء|ﺌ|ﺋ'\n",
        "    b0 = 'ئ'\n",
        "    c0 = sub(a0, b0, doc_string)\n",
        "    a1 = r'ﺎ|ﺄ|ٲ|ٱ|إ|ﺍ|أ'\n",
        "    a11 = r'ﺁ|آ'\n",
        "    b1 = r'ا'\n",
        "    b11 = r'آ'\n",
        "    c11 = sub(a11, b1, c0)\n",
        "    c1 = sub(a1, b1, c11)\n",
        "    a2 = r'ﺐ|ﺏ|ﺑ'\n",
        "    b2 = r'ب'\n",
        "    c2 = sub(a2, b2, c1)\n",
        "    a3 = r'ﭖ|ﭗ|ﭙ|ﺒ|ﭘ'\n",
        "    b3 = r'پ'\n",
        "    c3 = sub(a3, b3, c2)\n",
        "    a4 = r'ﭡ|ٺ|ٹ|ﭞ|ٿ|ټ|ﺕ|ﺗ|ﺖ|ﺘ|ﺜ'\n",
        "    b4 = r'ت'\n",
        "    c4 = sub(a4, b4, c3)\n",
        "    a5 = r'ﺙ|ﺛ'\n",
        "    b5 = r'ث'\n",
        "    c5 = sub(a5, b5, c4)\n",
        "    a6 = r'ﺞ|ﺝ|ڃ|ﺠ|ﺟ'\n",
        "    b6 = r'ج'\n",
        "    c6 = sub(a6, b6, c5)\n",
        "    a7 = r'ڃ|ﭽ|ﭼ|ﭻ'\n",
        "    b7 = r'چ'\n",
        "    c7 = sub(a7, b7, c6)\n",
        "    a8 = r'ﺢ|ﺤ|څ|ځ|ﺣ'\n",
        "    b8 = r'ح'\n",
        "    c8 = sub(a8, b8, c7)\n",
        "    a9 = r'ﺥ|ﺦ|ﺨ|ﺧ'\n",
        "    b9 = r'خ'\n",
        "    c9 = sub(a9, b9, c8)\n",
        "    a10 = r'ڏ|ډ|ﺪ|ﺩ'\n",
        "    b10 = r'د'\n",
        "    c10 = sub(a10, b10, c9)\n",
        "    a11 = r'ﺫ|ﺬ|ﻧ'\n",
        "    b11 = r'ذ'\n",
        "    c11 = sub(a11, b11, c10)\n",
        "    a12 = r'ۯ|ڙ|ڗ|ڒ|ڑ|ڕ|ﺭ|ﺮ'\n",
        "    b12 = r\"ر\"\n",
        "    c12 = sub(a12, b12, c11)\n",
        "    a13 = r\"ﺰ|ﺯ\"\n",
        "    b13 = r\"ز\"\n",
        "    c13 = sub(a13, b13, c12)\n",
        "    a14 = r\"ﮊ|ﮋ\"\n",
        "    b14 = r\"ژ\"\n",
        "    c14 = sub(a14, b14, c13)\n",
        "    a15 = r\"ݭ|ݜ|ﺱ|ﺲ|ښ|ﺴ|ﺳ\"\n",
        "    b15 = r\"س\"\n",
        "    c15 = sub(a15, b15, c14)\n",
        "    a16 = r\"ﺵ|ﺶ|ﺸ|ﺷ\"\n",
        "    b16 = r\"ش\"\n",
        "    c16 = sub(a16, b16, c15)\n",
        "    a17 = r\"ﺺ|ﺼ|ﺻ\"\n",
        "    b17 = r\"ص\"\n",
        "    c17 = sub(a17, b17, c16)\n",
        "    a18 = r\"ﺽ|ﺾ|ﺿ|ﻀ\"\n",
        "    b18 = r\"ض\"\n",
        "    c18 = sub(a18, b18, c17)\n",
        "    a19 = r\"ﻁ|ﻂ|ﻃ|ﻄ\"\n",
        "    b19 = r\"ط\"\n",
        "    c19 = sub(a19, b19, c18)\n",
        "    a20 = r\"ﻆ|ﻇ|ﻈ\"\n",
        "    b20 = r\"ظ\"\n",
        "    c20 = sub(a20, b20, c19)\n",
        "    a21 = r\"ڠ|ﻉ|ﻊ|ﻌ|ﻋ\"\n",
        "    b21 = r\"ع\"\n",
        "    c21 = sub(a21, b21, c20)\n",
        "    a22 = r\"ﻎ|ۼ|ﻍ|ﻐ|ﻏ\"\n",
        "    b22 = r\"غ\"\n",
        "    c22 = sub(a22, b22, c21)\n",
        "    a23 = r\"ﻒ|ﻑ|ﻔ|ﻓ\"\n",
        "    b23 = r\"ف\"\n",
        "    c23 = sub(a23, b23, c22)\n",
        "    a24 = r\"ﻘ|ﻕ|ڤ|ﻖ|ﻗ\"\n",
        "    b24 = r\"ق\"\n",
        "    c24 = sub(a24, b24, c23)\n",
        "    a25 = r\"ڭ|ﻚ|ﮎ|ﻜ|ﮏ|ګ|ﻛ|ﮑ|ﮐ|ڪ|ك|ﻙ\"\n",
        "    b25 = r\"ک\"\n",
        "    c25 = sub(a25, b25, c24)\n",
        "    a26 = r\"ﮚ|ﮒ|ﮓ|ﮕ|ﮔ\"\n",
        "    b26 = r\"گ\"\n",
        "    c26 = sub(a26, b26, c25)\n",
        "    a27 = r\"ﻟ|ﻝ|ﻞ|ﻠ|ڵ\"\n",
        "    b27 = r\"ل\"\n",
        "    c27 = sub(a27, b27, c26)\n",
        "    a28 = r\"ﻡ|ﻤ|ﻢ|ﻣ\"\n",
        "    b28 = r\"م\"\n",
        "    c28 = sub(a28, b28, c27)\n",
        "    a29 = r\"ڼ|ﻦ|ﻥ|ﻨ\"\n",
        "    b29 = r\"ن\"\n",
        "    c29 = sub(a29, b29, c28)\n",
        "    a30 = r\"ۆ|ވ|ﯙ|ۈ|ۋ|ﺆ|ۊ|ۇ|ۏ|ۅ|ۉ|ﻭ|ﻮ|ؤ|ٶ|ﺅ\"\n",
        "    b30 = r\"و\"\n",
        "    c30 = sub(a30, b30, c29)\n",
        "    a31 = r\"ﺔ|ﻬ|ھ|ﻩ|ﻫ|ﻪ|ۀ|ە|ة|ہ|ۃ\"\n",
        "    b31 = r\"ه\"\n",
        "    c31 = sub(a31, b31, c30)\n",
        "    a32 = r\"ئ|ﭛ|ﻯ|ۍ|ﻰ|ﻱ|ﻲ|ں|ﻳ|ﻴ|ﯼ|ې|ﯽ|ﯾ|ﯿ|ێ|ے|ى|ي\"\n",
        "    b32 = r\"ی\"\n",
        "    c32 = sub(a32, b32, c31)\n",
        "    a33 = r'¬'\n",
        "    b33 = r'‌'\n",
        "    c33 = sub(a33, b33, c32)\n",
        "    # pa0 = r'•|·|●|·|・|∙|｡|ⴰ'\n",
        "    # pb0 = r'.'\n",
        "    # pc0 = sub(pa0, pb0, c33)\n",
        "    pa1 = r',|٬|٫|‚|，'\n",
        "    pb1 = r'،'\n",
        "    pc1 = sub(pa1, pb1, c33)\n",
        "    pa2 = r'ʕ'\n",
        "    pb2 = r'؟'\n",
        "    pc2 = sub(pa2, pb2, pc1)\n",
        "    na0 = r'۰|٠'\n",
        "    nb0 = r'0'\n",
        "    nc0 = sub(na0, nb0, pc2)\n",
        "    na1 = r'۱|١'\n",
        "    nb1 = r'1'\n",
        "    nc1 = sub(na1, nb1, nc0)\n",
        "    na2 = r'۲|٢'\n",
        "    nb2 = r'2'\n",
        "    nc2 = sub(na2, nb2, nc1)\n",
        "    na3 = r'۳|٣'\n",
        "    nb3 = r'3'\n",
        "    nc3 = sub(na3, nb3, nc2)\n",
        "    na4 = r'۴|٤'\n",
        "    nb4 = r'4'\n",
        "    nc4 = sub(na4, nb4, nc3)\n",
        "    na5 = r'۵'\n",
        "    nb5 = r'5'\n",
        "    nc5 = sub(na5, nb5, nc4)\n",
        "    na6 = r'۶|٦'\n",
        "    nb6 = r'6'\n",
        "    nc6 = sub(na6, nb6, nc5)\n",
        "    na7 = r'۷|٧'\n",
        "    nb7 = r'7'\n",
        "    nc7 = sub(na7, nb7, nc6)\n",
        "    na8 = r'۸|٨'\n",
        "    nb8 = r'8'\n",
        "    nc8 = sub(na8, nb8, nc7)\n",
        "    na9 = r'۹|٩'\n",
        "    nb9 = r'9'\n",
        "    nc9 = sub(na9, nb9, nc8)\n",
        "    ea1 = r'ـ|ِ|ُ|َ|ٍ|ٌ|ً|'\n",
        "    eb1 = r''\n",
        "    ec1 = sub(ea1, eb1, nc9)\n",
        "    Sa1 = r'( )+'\n",
        "    Sb1 = r' '\n",
        "    Sc1 = sub(Sa1, Sb1, ec1)\n",
        "    Sa2 = r'(\\n)+'\n",
        "    Sb2 = r'\\n'\n",
        "    Sc2 = sub(Sa2, Sb2, Sc1)\n",
        "    Sa3 = r'ﻻ|ﻼ'\n",
        "    Sb3 = r'لا'\n",
        "    Sc3 = sub(Sa3, Sb3, Sc2)\n",
        "    Sa4 = r'ﷲ'\n",
        "    Sb4 = r'الله'\n",
        "    Sc4 = sub(Sa4, Sb4, Sc3)\n",
        "    return sub(zero_space, '', Sc4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4MftAywVeAk"
      },
      "source": [
        ">Preparing data for training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B4BhpVStxuY"
      },
      "source": [
        "# import constants\n",
        "# !pip install pyunpack\n",
        "# from pyunpack import Archive\n",
        "\n",
        "def read_csv_train_data(doc_add):\n",
        "  \n",
        "    csv_data = pd.read_csv(doc_add, delimiter='\\t')\n",
        "\n",
        "    doc_arr = []\n",
        "    doc_id = 0\n",
        "\n",
        "    for index in range(csv_data.shape[0]):\n",
        "        text = normalizer(str(csv_data['text'][index]))+' '\n",
        "        sentences = sentence_formation(text)\n",
        "        for sentence in sentences:\n",
        "            doc_arr.append(sentence)\n",
        "        doc_id += 1\n",
        "        # if doc_id % 10000 == 0:\n",
        "        #     print(doc_id)\n",
        "    print('####   done reading file   ####')\n",
        "    return doc_arr\n",
        "\n",
        "\n",
        "def report_unique_character(doc_arr):\n",
        "    \"\"\"\n",
        "    :param doc_arr: doc's array \n",
        "    :return: chars a set of every used char in the training set.\n",
        "    \"\"\"\n",
        "    chars = set()\n",
        "    for news in doc_arr:\n",
        "        for ch in news[2:-2]:\n",
        "            chars.add(ch)\n",
        "    print('unique chars number:\\t{}'.format(len(chars)))\n",
        "    return chars\n",
        "\n",
        "\n",
        "def tokenizer(doc_arr):\n",
        "    try:\n",
        "        char2index = dict()\n",
        "        char_limit = 0\n",
        "        sentences_arr = list()\n",
        "        # save char2index dictionary to use it to index docs\n",
        "        with open(document_root_path + 'char2index.pickle', 'rb') as char2index_file:\n",
        "            char2index = pickle.load(char2index_file)\n",
        "\n",
        "        while True:\n",
        "            avg_len = int(sum([len(news.split(' ')) for news in doc_arr]) / len(doc_arr))\n",
        "            # extract sentences that have less or equal word  to avg_len\n",
        "            for sentence in doc_arr:\n",
        "                if len(sentence.split()) <= avg_len:\n",
        "                    sentences_arr.append(sentence)\n",
        "                    # find max char in those sentences\n",
        "                    if len(sentence) > char_limit:\n",
        "                        char_limit = len(sentence)\n",
        "            break\n",
        "        print('####   start indexing   ####')\n",
        "        \n",
        "        # allocate a matrix for indexed news sentences\n",
        "        indexed_news = np.zeros((len(sentences_arr), char_limit), dtype=int)\n",
        "        # for each sentence, put their chars index in the indexed_news\n",
        "        for line, sentence in enumerate(sentences_arr):\n",
        "            indexed_news[line][0] = char2index['\\\\n']\n",
        "            for position, char in enumerate(sentence[2:-2]):\n",
        "                indexed_news[line][position + 1] = char2index[char]\n",
        "            indexed_news[line][len(sentence[2:-2]) + 1] = char2index['\\\\t']\n",
        "            # if line % 10000 == 0:\n",
        "            #     print(line)\n",
        "\n",
        "        print('####   done indexing   ####')\n",
        "\n",
        "        # write the indexed news on disk for further use\n",
        "        with open(document_root_path + 'indexed_train.pickle', 'wb') as indexed_train_file:\n",
        "            pickle.dump(indexed_news, indexed_train_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print('####   saved indexed file    #####')\n",
        "    except IOError:  # there are no char2index and index2char dictionary on disk, so compute it first.\n",
        "        # get texts char\n",
        "        chars = report_unique_character(doc_arr)\n",
        "        chars = list(chars)\n",
        "        # add start and end sign to the char index dict\n",
        "        chars.append('\\\\n')\n",
        "        chars.append('\\\\t')\n",
        "        # sort it\n",
        "        chars = sorted(chars)\n",
        "        # construct the char2index and index2char dict\n",
        "        char2index = {ch: i for i, ch in enumerate(chars)}\n",
        "        index2char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "        # save them on the disk\n",
        "        with open(document_root_path + 'index2char.pickle', 'wb') as index2char_file:\n",
        "            pickle.dump(index2char, index2char_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(document_root_path + 'char2index.pickle', 'wb') as char2index_file:\n",
        "            pickle.dump(char2index, char2index_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        # the requirements of this function are on the disk now, rerun the function to tokenize the news\n",
        "        tokenizer(doc_arr)\n",
        "\n",
        "\n",
        "def sentence_formation(news):\n",
        "    sentences = list()\n",
        "    end_points = sorted([r.start() for r in re.finditer('[.؟]', news)])\n",
        "    start_index = 0\n",
        "    for i in end_points:\n",
        "        sentences.append('\\\\n' + news[start_index:i + 2] + '\\\\t')\n",
        "        start_index = i + 2\n",
        "    return sentences\n",
        "\n",
        "\n",
        "\n",
        "# Archive('drive/MyDrive/Assignment_03/News.rar').extractall('drive/MyDrive/Assignment_03/data')\n",
        "\n",
        "tokenizer(read_csv_train_data(given_doc_root_path+'train.csv'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i9-JzdfIb2k"
      },
      "source": [
        "Define the Model and Train it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exn0uCnpwNsh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "22cf02a8-f8e2-4a2a-a25d-af7c1912d38c"
      },
      "source": [
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# define the RNN class which will be used as the model\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, input_size)\n",
        "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input_seq, hidden_state):\n",
        "        embedding = self.embedding(input_seq)\n",
        "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
        "        output = self.decoder(output)\n",
        "        return output, (hidden_state[0].detach(), hidden_state[1].detach())\n",
        "    \n",
        "    \n",
        "def train():\n",
        "    ########### Hyperparameters ###########\n",
        "    hidden_size = 256   # size of hidden state\n",
        "    num_layers = 2      # num of layers in LSTM layer stack\n",
        "    lr = 0.001          # learning rate\n",
        "    epochs = 10         # max number of epochs\n",
        "    load_chk = False   # load weights from save_path directory to continue training\n",
        "    saved_model_path = document_root_path+'CharRNN_5.pth'\n",
        "    saved_optimizer_path = document_root_path+'Evaluation_5.pth'\n",
        "    #######################################\n",
        "   \n",
        "    # load char2index and index2char dict() using saved pickle files\n",
        "    char2index = dict()\n",
        "    index2char = dict()\n",
        "    with open(document_root_path+'char2index.pickle','rb') as char2index_file:\n",
        "      char2index = pickle.load(char2index_file,encoding='utf8')\n",
        "    with open(document_root_path+'index2char.pickle', 'rb') as index2char_file:\n",
        "      index2char = pickle.load(index2char_file,encoding='utf8')\n",
        "    \n",
        "    # a list of character used in the text\n",
        "    chars = list(char2index.keys())\n",
        "    # number of all used characters\n",
        "    vocab_size = len(chars)\n",
        "\n",
        "    # load indexed data which has been saved on disk (here would be google drive)\n",
        "    with open(document_root_path+'indexed_train.pickle', 'rb') as indexed_data_file:\n",
        "      data = pickle.load(indexed_data_file)\n",
        "\n",
        "    n= len(data)\n",
        "\n",
        "    # construct an instance of the RNN model\n",
        "    rnn = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "    if load_chk:\n",
        "      rnn.load_state_dict(torch.load(saved_model_path))\n",
        "      print(\"Model loaded successfully !!\")\n",
        "\n",
        "    # loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
        "    # load optimizer if load_chk is True\n",
        "    if load_chk:\n",
        "      optimizer.load_state_dict(torch.load(saved_optimizer_path))\n",
        "      print(\"Optimizer loaded successfully\")\n",
        "\n",
        "    \n",
        "    # # training loop\n",
        "    for i_epoch in range(1, epochs+1):\n",
        "      start = time.time()\n",
        "        \n",
        "      running_loss = 0\n",
        "      # initial hiden state\n",
        "      hidden_state = None\n",
        "      \n",
        "\n",
        "      for line,sentence in enumerate(data):\n",
        "        # 2 array represent input string and the target string\n",
        "        input_seq = torch.tensor(sentence[:-1]).to(device)\n",
        "        target_seq = torch.tensor(sentence[1:]).to(device)\n",
        "        # chanbe their dimention to be capable to be used as model and loss function input\n",
        "        input_seq = torch.unsqueeze(input_seq,dim=1)\n",
        "        target_seq = torch.unsqueeze(target_seq,dim=1)          \n",
        "        \n",
        "        # forward pass\n",
        "        output, hidden_state = rnn(input_seq, hidden_state)\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_fn(torch.squeeze(output), torch.squeeze(target_seq))\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        # compute gradients and take optimizer step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # see model performance while training one epoch\n",
        "        if line %100000 ==0 and line!=0:\n",
        "          print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(line, running_loss/line))\n",
        "            \n",
        "        \n",
        "      # print loss and save weights after every epoch\n",
        "      print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(i_epoch, running_loss/n))\n",
        "      torch.save(rnn.state_dict(), document_root_path+'CharRNN_{}.pth'.format(i_epoch))\n",
        "      torch.save(optimizer.state_dict(), document_root_path+'Optimizer_{}.pth'.format(i_epoch))\n",
        "      print('time:  ',time.time()-start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded successfully !!\n",
            "Optimizer loaded successfully\n",
            "Epoch: 100000 \t Loss: 0.50616135\n",
            "Epoch: 200000 \t Loss: 0.48187288\n",
            "Epoch: 300000 \t Loss: 0.49679281\n",
            "Epoch: 400000 \t Loss: 0.51081238\n",
            "Epoch: 500000 \t Loss: 0.51839627\n",
            "Epoch: 600000 \t Loss: 0.48678916\n",
            "Epoch: 1 \t Loss: 0.48583660\n",
            "time:   9960.526891231537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-aeb812c8390f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-aeb812c8390f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-aeb812c8390f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, hidden_state)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 582\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}