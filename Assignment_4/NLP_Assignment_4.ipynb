{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QTVep7lX8iBV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM6B_r46uYXt"
      },
      "source": [
        "# Assignment 04: Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXq_v70vugsy"
      },
      "source": [
        "## Task 01: Answer's Sentnece Detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK0QjkBmuG0R"
      },
      "source": [
        "### Loading required packages and Get Access to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBE88JAdv4E9"
      },
      "source": [
        "get access to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2-T9yPwRFD-",
        "outputId": "ee6928d3-5819-4711-c6f1-58feb58b2222"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnJzfOdU26-k"
      },
      "source": [
        "* unzip data\r\n",
        "(uncomment below code if you run the code for the firts time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8BtzGFD0B7Y"
      },
      "source": [
        "copied_data_path = 'drive/MyDrive/squadv1.zip' \r\n",
        "# !unzip copied_data_path -d 'drive/MyDrive/Colab Notebooks/data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2VkYYvDy9ZY"
      },
      "source": [
        "Using Stanford [stanza](https://stanfordnlp.github.io/stanza/)   for POS, NER and Dependency Parsing in section one of the assignment, as it's claimed it's been implemented by the state-of-the-art NLP models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBlcTO3I05fn"
      },
      "source": [
        "\r\n",
        "*   Installing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa39OxwGsMxl"
      },
      "source": [
        "# Install stanza\r\n",
        "!pip install stanza\r\n",
        "# install sentence-transformer for vectorizing sentences\r\n",
        "!pip install -U sentence-transformers\r\n",
        "\r\n",
        "# install pickle to save objects's files\r\n",
        "! pip install pickle5\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txw1tWkg1Kx-"
      },
      "source": [
        "*   Setting up Stanford CoreNLP "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0buw-GAj1bLJ",
        "outputId": "519b953d-c89e-46ad-b97d-6976514cd9ef"
      },
      "source": [
        "# Import stanza\r\n",
        "import stanza\r\n",
        "# Download the Stanford CoreNLP package with Stanza's installation command\r\n",
        "# This'll take several minutes, depending on the network speed\r\n",
        "corenlp_dir = './corenlp'\r\n",
        "stanza.install_corenlp(dir=corenlp_dir)\r\n",
        "\r\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\r\n",
        "import os\r\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-15 17:12:20 INFO: Installing CoreNLP package into ./corenlp...\n",
            "Downloading http://nlp.stanford.edu/software/stanford-corenlp-latest.zip: 100%|██████████| 505M/505M [03:52<00:00, 2.17MB/s]\n",
            "2021-01-15 17:16:15 WARNING: For customized installation location, please set the `CORENLP_HOME` environment variable to the location of the installation. In Unix, this is done with `export CORENLP_HOME=./corenlp`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_965W-yxJy1"
      },
      "source": [
        "### Constants definition\r\n",
        "this section includes constants variables such as training data path or importants tags which are used in POS tagging or shuch as these variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHYP5qNKxXV5"
      },
      "source": [
        "data_path = 'drive/MyDrive/Colab Notebooks/data/squadv1/'\r\n",
        "train_file = 'train-v1.1.json'\r\n",
        "eval_file = 'dev-v1.1.json'\r\n",
        "documents_path = 'drive/MyDrive/Colab Notebooks/documents/'\r\n",
        "\r\n",
        "\r\n",
        "# set of pos tags which are more important in query\r\n",
        "POS_tags = {'NN', 'NNS', 'NNP', 'NNPS',         # nouns\r\n",
        "            'JJ', 'JJR', 'JJS',                 # adjectives\r\n",
        "            'VB','VBS','VBG','VBN','VBP','VBZ', # verbs\r\n",
        "            'WP','WP','WP$','WRB',              # WH terms\r\n",
        "            'CD'                                # numbers\r\n",
        "            }\r\n",
        "\r\n",
        "# use this model for vectorizing sentences in SentenceTransformer\r\n",
        "sentence_vectorizing_model = 'stsb-distilbert-base'\r\n",
        "\r\n",
        "# WH words which we want to detect in questions\r\n",
        "# also assign an id to each of them to be able to determin them later\r\n",
        "wh_terms = {'why':0, 'where':1 ,'when':2 ,'what':3 ,'who':4}\r\n",
        "\r\n",
        "# key of linguistic features \r\n",
        "# with their id (use it as a reffering index in represented list) \r\n",
        "# 0: locations 1: date/time 2: other name enteties\r\n",
        "linguistic_features = {'CITY': 0, 'COUNTRY': 0, 'LOCATION': 0,  'ORGANIZATION': 0,\r\n",
        "                       'DATE': 1, 'TIME': 1,\r\n",
        "                       'RELIGION': 2, 'PERSON': 2,  'TITLE': 2,  'IDEOLOGY': 2, 'NATIONALITY': 2, 'STATE_OR_PROVINCE': 2, 'CAUSE_OF_DEATH': 2,  'CRIMINAL_CHARGE': 2}\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOz6RAhAwsXN"
      },
      "source": [
        "### Answer_Sentnce_Detector class implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fHITB4PI6wN"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   import requirements\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iwYdkgOI5_B"
      },
      "source": [
        "import json\r\n",
        "import pickle5 as pickle\r\n",
        "\r\n",
        "from stanza.server import CoreNLPClient\r\n",
        "from sentence_transformers import SentenceTransformer\r\n",
        "import torch\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "from numpy import linalg as LA\r\n",
        "import numpy as np\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl_UbTTP8MyA"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   for more details on stanza NLP Models check [here](https://colab.research.google.com/github/stanfordnlp/stanza/blob/master/demo/Stanza_CoreNLP_Interface.ipynb#scrollTo=lIO4B5d6Rk4I)\r\n",
        "*   for more details on DistilBERT model used for vectorizing used sentence-transformer represented by [UKPLab](https://github.com/UKPLab/sentence-transformers)\r\n",
        "used 'stsb-distilbert-base' model as it's based on a transformer model and it's been one of the best model considering performance and speed based on the  [given results](https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit#gid=0)\r\n",
        "* in root extraction used lemmanizer instead of stemmer as it performe bether\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0DSRuyRQY-n"
      },
      "source": [
        "class Answer_Sentence_Detector():\r\n",
        "  \"\"\" a class to detect answer sentence among all sentence of a paragraph using sentence-question features\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self,train_data_path,k):\r\n",
        "      with open(train_data_path, 'r',encoding='utf8') as train_file:\r\n",
        "        # json file is a dictionary with keys 'data' and 'version'\r\n",
        "        data = json.load(train_file)['data']\r\n",
        "\r\n",
        "        # a dictionary that wach key is a doc_id and it's value is doc paraghraph text\r\n",
        "        self.paragraph_dict = dict()\r\n",
        "        # a dictionary that each key has a list ,which are in the same order,\r\n",
        "        # including question's data and answer and question's paraghraph id in order\r\n",
        "        self.question_answer_dict = {'question':[],\r\n",
        "                                      'answer':[],\r\n",
        "                                      'paragraph_id':[]\r\n",
        "                                     }\r\n",
        "        # use it for indexing paraghraphs\r\n",
        "        doc_id =0\r\n",
        "\r\n",
        "        for title_docs in data:\r\n",
        "          for doc in title_docs['paragraphs']:\r\n",
        "            self.paragraph_dict.update({doc_id:doc['context']})\r\n",
        "            for qas in doc['qas']:\r\n",
        "              for answer in qas['answers']:\r\n",
        "                self.question_answer_dict['question'].append(qas['question'])\r\n",
        "                # store answer_start too, as we need the start index to find the sentence that asnwer is in\r\n",
        "                self.question_answer_dict['answer'].append(answer)\r\n",
        "                self.question_answer_dict['paragraph_id'].append(doc_id)\r\n",
        "            doc_id+=1\r\n",
        "      # call preprocessor\r\n",
        "      self.preprocess(k)\r\n",
        "\r\n",
        "  def preprocess(self,k):\r\n",
        "\r\n",
        "    print('start peprocessor')\r\n",
        "\r\n",
        "    # ######## remove paraghraphs and related property which has more than k sentnces ########\r\n",
        "\r\n",
        "    # the index of current doc index in the paraghraph's id array\r\n",
        "    doc_id_index =0\r\n",
        "    # the index of current doc\r\n",
        "    doc_id =0\r\n",
        "\r\n",
        "    while doc_id < len(self.paragraph_dict): # for each paragraph\r\n",
        "      # some sentences starts with \\n, remove all \\n as we use it to seperate sentences and these are redundant\r\n",
        "      self.paragraph_dict[doc_id] = self.paragraph_dict[doc_id].replace('\\n','') \r\n",
        "\r\n",
        "      # get number of sentence each doc has\r\n",
        "      doc_sentences_len = len(self.sent_tokenize((self.paragraph_dict[doc_id])))\r\n",
        "      if doc_sentences_len > k:   # if the current doc's sentence is more than k\r\n",
        "        # then remove it from self.doc_dict and also \r\n",
        "        # remove related questions and answers from self.question_answer_dict\r\n",
        "\r\n",
        "        # remove paraghraph \r\n",
        "        self.paragraph_dict.pop(doc_id)\r\n",
        "\r\n",
        "        # remove related questions and answers\r\n",
        "        while self.question_answer_dict['paragraph_id'][doc_id_index]<=doc_id:\r\n",
        "          if self.question_answer_dict['paragraph_id'][doc_id_index]<doc_id:\r\n",
        "            doc_id_index+=1\r\n",
        "          else:\r\n",
        "            self.question_answer_dict['question'].pop(doc_id_index)\r\n",
        "            self.question_answer_dict['answer'].pop(doc_id_index)\r\n",
        "            self.question_answer_dict['paragraph_id'].pop(doc_id_index)\r\n",
        "      \r\n",
        "      doc_id+=1\r\n",
        "    \r\n",
        "    # call self_.POS_tagger for all questions and return refrmed ones\r\n",
        "    # (it's been explain in the function the way it changes them)\r\n",
        "\r\n",
        "    # replace questions with refrmed ones\r\n",
        "    self.question_answer_dict['question'] = self.POS_tagger(self.question_answer_dict['question'])\r\n",
        "    print('finished preproccessor')\r\n",
        "  \r\n",
        "  def POS_tagger(self, questions_list):\r\n",
        "    edited_questions = list()\r\n",
        "    # define CoreNLPClient based on our usaage, \r\n",
        "    # and use it to select those words with tags in POS_tags \r\n",
        "    with CoreNLPClient(\r\n",
        "          annotators=['pos','lemma'],\r\n",
        "          timeout=30000,\r\n",
        "          memory='4G', endpoint='http://localhost:9001', be_quiet=True) as client:\r\n",
        "          \r\n",
        "          for index,question in enumerate(questions_list):\r\n",
        "            nn = client.annotate(question)\r\n",
        "            new_question = ''\r\n",
        "            old_question = ''\r\n",
        "            for sentence in nn.sentence:\r\n",
        "              for token in sentence.token:\r\n",
        "                # print('{:12s}\\t{:6s}'.format(token.value, token.pos),end='\\t')\r\n",
        "                if token.pos in POS_tags:\r\n",
        "                  # print(True,end='')\r\n",
        "                  if token.pos =='NNP' or token.pos =='NNPS':\r\n",
        "                    # add lemmanized term to new_question\r\n",
        "                    new_question += token.lemma + ' '\r\n",
        "                  old_question += token.lemma + ' '\r\n",
        "\r\n",
        "            # if this question doesn't contain that much token \r\n",
        "            # of speccified POS tags, do not change it\r\n",
        "            if len(new_question.split()) > 1:\r\n",
        "              edited_questions.append(new_question)\r\n",
        "            else: \r\n",
        "              edited_questions.append(old_question)\r\n",
        "            \r\n",
        "            if index % 10000 == 0 and index!= 0:\r\n",
        "              print('*POS_tagger* processed {:16d}\\tover\\t{:16d} qustions'.format(index, len(questions_list)))\r\n",
        "    return edited_questions\r\n",
        "\r\n",
        "  def sent_tokenize(self, paragraph):\r\n",
        "    # a list of all sentnece in the given praghraph\r\n",
        "    sentence_list = list()\r\n",
        "    \r\n",
        "    # add '\\n' at the end of each sentence   \r\n",
        "    reformed_paragraph = paragraph.replace('. ','. \\n').replace('? ','? \\n')\r\n",
        "\r\n",
        "    # then split by '\\n' and add each sentence to the list\r\n",
        "    for sentence in reformed_paragraph.split('\\n'):\r\n",
        "      if len(sentence) > 0:\r\n",
        "        sentence_list.append(sentence)\r\n",
        "\r\n",
        "    return sentence_list  # return a list of given paraghraph's sentences\r\n",
        "\r\n",
        "  def sent_vectorize(self, paragraph_dict, question_answer_dict, train= True):\r\n",
        "    \"\"\" \r\n",
        "      vectorize data for train, evaluation and querys \r\n",
        "      \r\n",
        "      @param  paragraph_dict(dictionary): a dictionary like {'paragraph_id': 'paragraph',...}\r\n",
        "      @param  question_answer_dict(dictionary): a dictionary like {'question':['q1',...] ,...\r\n",
        "                                                                    'answer': ['a1', ...], ...\r\n",
        "                                                                    'paragraph_id': ['id1', ...]\r\n",
        "                                                                    }\r\n",
        "                                                                    \r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "    print('current available device is \"{}\" for vectorizing'.format(device))\r\n",
        "    vectorizing_model = SentenceTransformer(sentence_vectorizing_model, device=device)\r\n",
        "\r\n",
        "    sentences_vec = dict()\r\n",
        "    # use index to show how processed/all data\r\n",
        "    index =0\r\n",
        "    for doc_id, doc in zip(paragraph_dict.keys(), paragraph_dict.values()):\r\n",
        "\r\n",
        "      sentences_list = self.sent_tokenize(doc)\r\n",
        "      sentences_vec.update({doc_id:vectorizing_model.encode(sentences_list)})\r\n",
        "      if index %1000 ==0 and index != 0:\r\n",
        "        print('* vectorizing sentence * {:10s}:{:12d}  {:10s}  {:12d}'.format('processed',index,'over',len(paragraph_dict)))\r\n",
        "      index+=1\r\n",
        "    \r\n",
        "    questions_vec = list()\r\n",
        "\r\n",
        "    # use index to show how processed/all data\r\n",
        "    index = 0\r\n",
        "    for question in question_answer_dict['question']:\r\n",
        "      questions_vec.append(vectorizing_model.encode(question))\r\n",
        "\r\n",
        "      if index %100000 ==0 and index != 0:\r\n",
        "        print('* vectorizing question * {:10s}:{:12d}  {:10s}  {:12d}'.format('processed',index,'over',len(question_answer_dict['question'])))\r\n",
        "      index+=1\r\n",
        "    \r\n",
        "    if train: # if it's train data, save vectorized data\r\n",
        "      with open(documents_path + 'vectorized_train_data.pickle','wb') as vectorized_file:\r\n",
        "        pickle.dump({'doc_vec':sentences_vec,'question_vec':np.array(questions_vec)}, vectorized_file, protocol= pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    # return vectorized data\r\n",
        "    return {'doc_vec':sentences_vec,'question_vec':np.array(questions_vec)}\r\n",
        "  \r\n",
        "  def calculate_vector_distances(self, type, vectorized_data ,question_answer_dict, train= True ):\r\n",
        "    \"\"\"\r\n",
        "      @param  type(int): valid value for it is 1,2, or 3 which shows which type of distance calculator \r\n",
        "                        is going to be used\r\n",
        "                        type = 1 : cosine similarity\r\n",
        "                        type = 2 : Euclidean distance\r\n",
        "                        type = 3 : dot product    \r\n",
        "      @param  vectorized_data(dict): vectorized_data's structure is like: \r\n",
        "                                    {\r\n",
        "                                      'doc_vec': {\r\n",
        "                                                  'doc_id': sentences_vec,..\r\n",
        "                                                  }\r\n",
        "                                      'question_vec': questions_vec\r\n",
        "                                    }\r\n",
        "    @return distance_dict(dict): distance_dict's structure is like:\r\n",
        "      {'par_id':[\r\n",
        "                  ['first_question/first_sentence distance', 'first_question/second_sentence distance', ...]\r\n",
        "                  ['second_question/first_sentence distance', 'second_question/second_sentence distance', ...]\r\n",
        "                  ,...\r\n",
        "                ]\r\n",
        "        ,...\r\n",
        "      }\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "\r\n",
        "    # a variable to store distances \r\n",
        "    distance_dict = dict()\r\n",
        "    # size of all questions\r\n",
        "    questions_list_size = len(question_answer_dict['question']) \r\n",
        "\r\n",
        "    for index in range(questions_list_size):\r\n",
        "      # extract vector for this index's question\r\n",
        "      question_vec = vectorized_data['question_vec'][index]\r\n",
        "\r\n",
        "      # extract doc_id for this question\r\n",
        "      doc_id = question_answer_dict['paragraph_id'][index]\r\n",
        "\r\n",
        "      if doc_id not in distance_dict:\r\n",
        "        # add this doc to dictionary\r\n",
        "        distance_dict.update({doc_id:[]})\r\n",
        "\r\n",
        "      # a list of distance for this question and each sentence of it's doc(paragraph)\r\n",
        "      question_dictance = list()\r\n",
        "\r\n",
        "      # calculate distance based one the asked method\r\n",
        "      if type == 1:   # cosine similarity\r\n",
        "        question_distance = [np.dot(sentence_vec,question_vec)/\r\n",
        "                            (LA.norm(sentence_vec) * LA.norm(question_vec))\r\n",
        "                              for sentence_vec in vectorized_data['doc_vec'][doc_id]]\r\n",
        "      elif type == 2:   # Euclidean distnace\r\n",
        "        question_distance = [LA.norm(question_vec - sentence_vec)\r\n",
        "                              for sentence_vec in vectorized_data['doc_vec'][doc_id]]\r\n",
        "      elif type == 3:   # dot product\r\n",
        "        question_distance = [np.dot(sentence_vec,question_vec)\r\n",
        "                              for sentence_vec in vectorized_data['doc_vec'][doc_id]]\r\n",
        "      \r\n",
        "      # add distance to dictionary\r\n",
        "      distance_dict[doc_id].append(question_distance)\r\n",
        "\r\n",
        "      if index % 100000 == 0 and index != 0:\r\n",
        "        print('* calculate vector distance * {:10s}:{:12d}  {:10s}  {:12d}'.format('processed',index,'over',questions_list_size))\r\n",
        "    \r\n",
        "    if train: # save data if it's called for training set\r\n",
        "      with open(documents_path+ 'distances_train_{}.pickle'.format(type), 'wb') as distances_file:\r\n",
        "       pickle.dump(distance_dict, distances_file, protocol= pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    return distance_dict\r\n",
        "\r\n",
        "  def compare_roots(self, paragraph_dict, question_answer_dict, train= True):\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "        compared_roots_dict's structure is like:\r\n",
        "        {'par_id':[\r\n",
        "                    ['first_question/first_sentence root_sim', 'first_question/second_sentence root_sim', ...]\r\n",
        "                    ['second_question/first_sentence root_sim', 'second_question/second_sentence root_sim', ...]\r\n",
        "                    ,...\r\n",
        "                  ]\r\n",
        "          ,...\r\n",
        "        }\r\n",
        "    \"\"\"\r\n",
        "    compared_roots_dict= dict()\r\n",
        "\r\n",
        "    with CoreNLPClient(\r\n",
        "    annotators=['tokenize','ssplit','pos','depparse','lemma']\r\n",
        "    , timeout=60000, memory='4G', endpoint='http://localhost:9001', be_quiet=True) as client:\r\n",
        "      # submit the request to the server\r\n",
        "\r\n",
        "      question_index = 0\r\n",
        "      questions_size = len(question_answer_dict['question'])\r\n",
        "      # for each paragraph\r\n",
        "      for paragraph_id in paragraph_dict.keys():\r\n",
        "        # add this paragraph_id to compared_roots_dict\r\n",
        "        compared_roots_dict.update({paragraph_id: list() })\r\n",
        "        \r\n",
        "        # use self.sent_tokenize to get list of this sentence paragraph\r\n",
        "        sentences_list = self.sent_tokenize(paragraph_dict[paragraph_id])\r\n",
        "        \r\n",
        "        # now for eahc question that is for this paragrpah \r\n",
        "        while question_index <questions_size and question_answer_dict['paragraph_id'][question_index] == paragraph_id:\r\n",
        "          # add a list to stor this root-comparision for this question an each sentence of the pragraph\r\n",
        "          compared_roots_dict[paragraph_id].append(list())\r\n",
        "\r\n",
        "          # get question text\r\n",
        "          question = question_answer_dict['question'][question_index]\r\n",
        "          # construct an annotate object for question text\r\n",
        "          question_ann = client.annotate(question)\r\n",
        "          # get the first sentence for question (as it contains only one question)\r\n",
        "          que = question_ann.sentence[0]\r\n",
        "          # get the root of the dependency parse for question \r\n",
        "          question_root = que.basicDependencies.root[0]\r\n",
        "          # get roots lemma\r\n",
        "          question_root_lemma = que.token[question_root-1].lemma\r\n",
        "\r\n",
        "          for sentence in sentences_list: # for each sentence in sentence list\r\n",
        "            sentence_ann = client.annotate(sentence)\r\n",
        "            # same_root defaults value is false, and turn it to true when similar root's been seen\r\n",
        "            same_root =False\r\n",
        "            for sen in sentence_ann.sentence:\r\n",
        "              # get the root of the dependency parse for sentence\r\n",
        "              sentence_root = sen.basicDependencies.root[0]\r\n",
        "              # get roots lemma\r\n",
        "              sentence_root_lemma = sen.token[sentence_root-1].lemma\r\n",
        "              # compare roots\r\n",
        "              if question_root_lemma == sentence_root_lemma:\r\n",
        "                same_root = True\r\n",
        "                break\r\n",
        "            \r\n",
        "            # now save 1 if the question ans sentence both have same root \r\n",
        "            if same_root:\r\n",
        "              compared_roots_dict[paragraph_id][-1].append(1)\r\n",
        "            else:\r\n",
        "              compared_roots_dict[paragraph_id][-1].append(0)\r\n",
        "\r\n",
        "\r\n",
        "          if question_index % 10000 == 0 and question_index != 0:\r\n",
        "            print('* root comprision * {:10s}:{:12d}  {:10s}  {:12d}'.format('processed',question_index,'over',questions_size))\r\n",
        "          question_index += 1                \r\n",
        "\r\n",
        "    if train: # save file if it's called for training set\r\n",
        "      with open(documents_path+ 'train_compared_roots.pickle', 'wb') as compared_roots_file:\r\n",
        "        pickle.dump(compared_roots_dict, compared_roots_file, protocol= pickle.HIGHEST_PROTOCOL)\r\n",
        "    # return result as a dictiounary\r\n",
        "    return compared_roots_dict\r\n",
        "      \r\n",
        "  def check_wh_presence(self,question_answer_dict, train= True):\r\n",
        "    # a list to store wh presence\r\n",
        "    wh_feature_list = list()\r\n",
        "\r\n",
        "    for index, question in enumerate(question_answer_dict['question']):\r\n",
        "      # make a list of size the wh features in the wh_terms defined in the constants cell\r\n",
        "      wh_feature = [0 for i in range(len(wh_terms))]\r\n",
        "      # split questions to list of tokens\r\n",
        "      question_list =  question.split()\r\n",
        "      # for each token in question\r\n",
        "      for token in question_list:\r\n",
        "        # check if it's a wh term \r\n",
        "        if token in wh_terms:\r\n",
        "          # if it is, change this wh term index to 1\r\n",
        "          wh_feature[wh_terms[token]] = 1\r\n",
        "      #add wh_feature list for this question to list of all wh_features    \r\n",
        "      wh_feature_list.append(wh_feature)\r\n",
        "\r\n",
        "      if index % 10000 == 0 and index != 0:\r\n",
        "          print('* root comprision * {:10s}:{:12d}  {:10s}  {:12d}'.format('processed',index,'over',len(question_answer_dict['question'])))\r\n",
        "\r\n",
        "    if train:    # save data if it's called for training set  \r\n",
        "      with open(documents_path + 'train_wh_feature.pickle' , 'wb') as wh_feature_file:\r\n",
        "        pickle.dump(wh_feature_list, wh_feature_file, protocol= pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    return wh_feature_list\r\n",
        "      \r\n",
        "  def extract_linguistic_features(self,paragraph_dict, question_answer_dict, train= True):\r\n",
        "\r\n",
        "    features_num = np.unique(np.array(list(linguistic_features.values()))).shape[0]\r\n",
        "    \r\n",
        "    linguistic_feature_dict = {'paragraph':dict(),\r\n",
        "                          'question': []}\r\n",
        "\r\n",
        "    with CoreNLPClient(\r\n",
        "        annotators=['ner'],\r\n",
        "        timeout=30000,\r\n",
        "        memory='4G', endpoint='http://localhost:9001', be_quiet=True) as client:\r\n",
        "\r\n",
        "        index = 0\r\n",
        "        # first calculate for sentences in paragraphs\r\n",
        "        for paragraph_id, paragraph in zip(paragraph_dict.keys(),paragraph_dict.values()):\r\n",
        "          # first add this paragraph id to linguistic_feature_dict\r\n",
        "          linguistic_feature_dict['paragraph'].update({paragraph_id:list()})\r\n",
        "\r\n",
        "          # now cal linguistic feature for each sentence of it \r\n",
        "          \r\n",
        "          sentences_list = self.sent_tokenize(paragraph)\r\n",
        "\r\n",
        "          for sentence in sentences_list:\r\n",
        "            \r\n",
        "            # first construct a list that each of it's cell represent for one \r\n",
        "            # of the aske NER (based on the id that's been given to them in constants cell ) \r\n",
        "            ling_feature_list = [0 for i in range(features_num)]\r\n",
        "\r\n",
        "            ann=client.annotate(sentence)\r\n",
        "\r\n",
        "            for i,sen in enumerate(ann.sentence):\r\n",
        "              for term in sen.token:\r\n",
        "                ner = term.ner\r\n",
        "                if ner in linguistic_features.keys():\r\n",
        "                  ling_feature_list[linguistic_features[ner]] += 1\r\n",
        "            \r\n",
        "            # add this sentence liguistic feature\r\n",
        "            linguistic_feature_dict['paragraph'][paragraph_id].append(ling_feature_list)\r\n",
        "          \r\n",
        "          if index %1000 ==0 and index!= 0:\r\n",
        "            print('* linguistic sentence * {:10s}:{:12d}  {:10s}  {:12d}'.format('processed',index,'over',len(paragraph_dict)))\r\n",
        "          index += 1\r\n",
        "\r\n",
        "        index = 0\r\n",
        "        for question in question_answer_dict['question']:\r\n",
        "\r\n",
        "          # first construct a list that each of it's cell represent for one \r\n",
        "          # of the aske NER (based on the id that's been given to them in constants cell ) \r\n",
        "          ling_feature_list = [0 for i in range(features_num)]\r\n",
        "\r\n",
        "          ann=client.annotate(question)\r\n",
        "\r\n",
        "          for i,sen in enumerate(ann.sentence):\r\n",
        "            for term in sen.token:\r\n",
        "              ner = term.ner\r\n",
        "              if ner in linguistic_features.keys():\r\n",
        "                ling_feature_list[linguistic_features[ner]] += 1\r\n",
        "          \r\n",
        "          # add this sentence liguistic feature\r\n",
        "          linguistic_feature_dict['question'].append(ling_feature_list)\r\n",
        "          if index %10000 ==0 and index !=0 :\r\n",
        "            print('* linguistic question * {:10s}:{:12d}  {:10s}  {:12d}'.format('processed',index,'over',len(question_answer_dict['question'])))\r\n",
        "          index += 1\r\n",
        "\r\n",
        "    if train: \r\n",
        "      with open(documents_path + 'train_linguistic_feature.pickle', 'wb') as linguistic_feature_file:\r\n",
        "        pickle.dump(linguistic_feature_dict, linguistic_feature_file, protocol= pickle.HIGHEST_PROTOCOL)\r\n",
        "      \r\n",
        "    return linguistic_feature_dict\r\n",
        "\r\n",
        "  def transform_data(self, sentence_linguistic_feature= None, \r\n",
        "                     cosine_similarity_dict = None, euclidean_distance_dict = None,\r\n",
        "                     dot_product_dict = None, root_comparision_dict = None,\r\n",
        "                     wh_presence_list = None, question_linguistic_feature = None, train= True):\r\n",
        "    \"\"\"\r\n",
        "      for evaluation,questy parts get these variables, for train, load them\r\n",
        "\r\n",
        "      we vectorize every extracted features, which would be like dit for every \r\n",
        "      pair of sentence,question\r\n",
        "      [<sentence_linguistic_feature>, <cosine similarity>, <Euclidean distance>,\r\n",
        "      <dot product>, <root comparision>, <wh_presence>, <question_linguistic_feature>\r\n",
        "      ]\r\n",
        "\r\n",
        "      and store it like below:\r\n",
        "      transformed_data = {<paragraph_id>: [\r\n",
        "                                            [ <first question,first sentence features>,\r\n",
        "                                              <first question,second sentence features>,...\r\n",
        "                                            ],\r\n",
        "                                            [ <second question,first sentence features>,\r\n",
        "                                              <second question,second sentence features>,...\r\n",
        "                                            ], ...\r\n",
        "                                          ],...\r\n",
        "                          }\r\n",
        "      each mentioned feature format:\r\n",
        "      <sentence_linguistic_feature>: a list of size 3 \r\n",
        "      <cosine similarity>: a float number\r\n",
        "      <Euclidean distance>: a float number\r\n",
        "      <dot product>: a float number\r\n",
        "      <root comparision>: an int (0 or 1)\r\n",
        "      <wh_presence>: a list of size 5 \r\n",
        "      <questoin_linguistic_feature>: a list of size 3  \r\n",
        "\r\n",
        "      so for just putting these all together a list of size 15 is required for each\r\n",
        "      sentence,question pairs\r\n",
        "      \r\n",
        "    \"\"\"\r\n",
        "    transformed_data = dict()\r\n",
        "    if train:\r\n",
        "      # vars to load fetures \r\n",
        "      sentence_linguistic_feature= dict()\r\n",
        "      cosine_similarity_dict = dict()\r\n",
        "      euclidean_distance_dict = dict()\r\n",
        "      dot_product_dict = dict()\r\n",
        "      root_comparision_dict = dict()\r\n",
        "      wh_presence_list = list()\r\n",
        "      question_linguistic_feature = list()\r\n",
        "    \r\n",
        "      # first open every file in order to organize fetures\r\n",
        "      try: # try to opnen distances file\r\n",
        "        with open(documents_path + 'distances_train_1.pickle', 'rb') as cosine_similarity_file:\r\n",
        "          cosine_similarity_dict = pickle.load(cosine_similarity_file)\r\n",
        "      \r\n",
        "        with open(documents_path + 'distances_train_2.pickle', 'rb') as euclidean_distnace_file:\r\n",
        "          euclidean_distance_dict = pickle.load(euclidean_distnace_file)\r\n",
        "\r\n",
        "        with open(documents_path + 'distances_train_3.pickle', 'rb') as dot_product_file:\r\n",
        "          dot_product_dict = pickle.load(dot_product_file)\r\n",
        "      except IOError: # calculate distances file\r\n",
        "        print('*'*8, 'some distances files for training do not exist, calculating them might take a while','*'*8)\r\n",
        "\r\n",
        "        try: # try to open vectorized data\r\n",
        "          vectorized_data = dict()\r\n",
        "          with open(documents_path + 'vectorized_train_data.pickle', 'rb') as vectorized_data_file:\r\n",
        "            vectorized_data = pickle.load(vectorized_data_file)\r\n",
        "        except IOError:\r\n",
        "          print('*'*8, 'vectorized data file does not exist, preparing it might take a while','*'*8)\r\n",
        "          vectorized_data = self.sent_vectorize(self.paragraph_dict, self.question_answer_dict)\r\n",
        "        # calculate distances\r\n",
        "        cosine_similarity_dict = self.calculate_vector_distances(1, vectorized_data, self.question_answer_dict)\r\n",
        "        euclidean_distance_dict = self.calculate_vector_distances(2, vectorized_data, self.question_answer_dict)\r\n",
        "        dot_product_dict = self.calculate_vector_distances(3, vectorized_data, self.question_answer_dict)\r\n",
        "\r\n",
        "      try: # try to open compared root file\r\n",
        "        with open(documents_path +'train_compared_roots.pickle', 'rb') as root_comparision_file:\r\n",
        "          root_comparision_dict = pickle.load(root_comparision_file)\r\n",
        "      except IOError:\r\n",
        "        print('*'*8,'roots comparision file for training does not exist, preparing it might take a while','*'*8)\r\n",
        "        root_comparision_dict= self.compare_roots(self.paragraph_dict, self.question_answer_dict)\r\n",
        "      \r\n",
        "      try:  # try to open wh features file\r\n",
        "        with open(documents_path + 'train_wh_feature.pickle', 'rb') as wh_feature_file:\r\n",
        "          wh_presence_list = pickle.load(wh_feature_file)\r\n",
        "      except IOError:\r\n",
        "        print('*'*8, 'wh_feature file for training does not exist, preparing it might take a while','*'*8)\r\n",
        "        wh_presence_list = self.check_wh_presence(self.question_answer_dict)\r\n",
        "      \r\n",
        "      try: # try to open linguistic feature file\r\n",
        "        with open(documents_path + 'train_linguistic_feature.pickle', 'rb') as linguistic_feature_file:\r\n",
        "          linguistic_feature = pickle.load(linguistic_feature_file)\r\n",
        "      except IOError:\r\n",
        "        print('*'*8, 'linguistic features file for training does not exist, preparing it might take a while','*'*8)\r\n",
        "        linguistic_feature = self.extract_linguistic_features(self.paragraph_dict, self.question_answer_dict)\r\n",
        "      sentence_linguistic_feature,question_linguistic_feature =linguistic_feature['paragraph'],linguistic_feature['question']\r\n",
        "      \r\n",
        "    # to sumup questions appeared by now\r\n",
        "    questions_index = 0\r\n",
        "    # as cosine_similarity_dict structure is like what we want for transformed_data\r\n",
        "    # use it to gather all data\r\n",
        "    for paragraph_id in cosine_similarity_dict.keys():\r\n",
        "      \r\n",
        "      transformed_data.update({paragraph_id: list() })\r\n",
        "      \r\n",
        "      for question_index in range(len(cosine_similarity_dict[paragraph_id])):\r\n",
        "        \r\n",
        "        # define a list containing all sentence/question pairs for one paragraph and question\r\n",
        "        transformed_data[paragraph_id].append( list() )\r\n",
        "        # paragraph_question_feature = list()\r\n",
        "\r\n",
        "        for sentence_index in range(len(cosine_similarity_dict[paragraph_id][question_index])):\r\n",
        "          \r\n",
        "          # paragraph_question_feature.append(list())\r\n",
        "          transformed_data[paragraph_id][-1].append( list() )\r\n",
        "\r\n",
        "          # 1: add sentence_linguistic_feature\r\n",
        "          transformed_data[paragraph_id][-1][-1] += sentence_linguistic_feature[paragraph_id][sentence_index]\r\n",
        "\r\n",
        "          # 2: add cosine similarity feature\r\n",
        "          transformed_data[paragraph_id][-1][-1].append(cosine_similarity_dict[paragraph_id][question_index][sentence_index])\r\n",
        "\r\n",
        "          # 3: add Euclidean distance feature\r\n",
        "          transformed_data[paragraph_id][-1][-1].append(euclidean_distance_dict[paragraph_id][question_index][sentence_index])\r\n",
        "\r\n",
        "          # 4: add dot product feature\r\n",
        "          transformed_data[paragraph_id][-1][-1].append(dot_product_dict[paragraph_id][question_index][sentence_index])\r\n",
        "\r\n",
        "          # 5: add root comparision feature \r\n",
        "          transformed_data[paragraph_id][-1][-1].append(root_comparision_dict[paragraph_id][question_index][sentence_index])\r\n",
        "\r\n",
        "          # 6: add wh presence feature\r\n",
        "          transformed_data[paragraph_id][-1][-1] += wh_presence_list[questions_index] \r\n",
        "\r\n",
        "          # 7: add question_linguistic_feature\r\n",
        "          transformed_data[paragraph_id][-1][-1] += question_linguistic_feature[questions_index]\r\n",
        "        \r\n",
        "        \r\n",
        "        questions_index +=1\r\n",
        "\r\n",
        "    if train:\r\n",
        "      with open(documents_path + 'train_transformed_data.pickle', 'wb') as transformed_data_file:\r\n",
        "        pickle.dump(transformed_data, transformed_data_file, protocol = pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    return transformed_data\r\n",
        "  \r\n",
        "  def extract_label(self, paragraph_dict, question_answer_dict):\r\n",
        "    \"\"\"\r\n",
        "      @return label_dict(dict): -\t{<paragraph_id>: [\r\n",
        "                                                    [ <first answer, first sentence label>,\r\n",
        "                                                      <first answer, second sentence label>, ...\r\n",
        "                                                    ],\r\n",
        "                                                    [ <second answer, first sentence label>,\r\n",
        "                                                      <second answer, second sentence label>, ...\r\n",
        "                                                      ], ...\r\n",
        "                                                    ],...\r\n",
        "                                  \t}\r\n",
        " \r\n",
        "    \"\"\"\r\n",
        "    answer_index = 0\r\n",
        "    answers_size = len(question_answer_dict['answer'])\r\n",
        "    label_dict = dict()\r\n",
        "    for paragraph_id in paragraph_dict.keys():\r\n",
        "\r\n",
        "      label_dict.update({paragraph_id: list()})\r\n",
        "      sentences_list = self.sent_tokenize(paragraph_dict[paragraph_id])\r\n",
        "      \r\n",
        "      while answer_index < answers_size and question_answer_dict['paragraph_id'][answer_index] == paragraph_id:\r\n",
        "      \r\n",
        "        label_dict[paragraph_id].append(list())\r\n",
        "        answer_start_index = question_answer_dict['answer'][answer_index]['answer_start']\r\n",
        "        char_index = 0\r\n",
        "        for sentence in sentences_list:\r\n",
        "          if char_index < answer_start_index <= char_index + len(sentence):\r\n",
        "            label_dict[paragraph_id][-1].append(1)\r\n",
        "          else:\r\n",
        "            label_dict[paragraph_id][-1].append(0)\r\n",
        "          \r\n",
        "          char_index += len(sentence)\r\n",
        "      \r\n",
        "        if answer_index % 10000 ==0:\r\n",
        "          print('* transform data * {:10s}:{:12d}  {:10s}  {:12d}'.format('processed',answer_index,'over',answers_size))\r\n",
        "        answer_index += 1\r\n",
        "\r\n",
        "    return label_dict \r\n",
        "      \r\n",
        "  def prepare_data_for_classifier(self, transformed_data, label):\r\n",
        "\r\n",
        "    x_vec = list()\r\n",
        "    y_vec = list()\r\n",
        "    for paragraph_id in transformed_data.keys():\r\n",
        "      for question_id in range(len(transformed_data[paragraph_id])): \r\n",
        "        for sentence_id in range(len(transformed_data[paragraph_id][question_id])):\r\n",
        "          # add sentence-question paird feature vector to x_vec\r\n",
        "          x_vec.append(transformed_data[paragraph_id][question_id][sentence_id])\r\n",
        "          # add label for this sentence-question pair vector to y_vec \r\n",
        "          y_vec.append(label[paragraph_id][question_id][sentence_id])\r\n",
        "\r\n",
        "    # as our model get np.array as input, make these list object a np.array object\r\n",
        "    x_vec = np.array(x_vec)\r\n",
        "    y_vec = np.array(y_vec)\r\n",
        "\r\n",
        "    return x_vec, y_vec # rturn np.arrays as vectors \r\n",
        "\r\n",
        "  def fit(self, transformed_data_dict):\r\n",
        "    \r\n",
        "    label_dict = self.extract_label(self.paragraph_dict, self.question_answer_dict)\r\n",
        "    # print(label_dict.keys())\r\n",
        "    X, Y = self.prepare_data_for_classifier(transformed_data_dict, label_dict)\r\n",
        "    \r\n",
        "\r\n",
        "    # create an object from sklearn GaussinaNB for classification tast\r\n",
        "    clf = GaussianNB()\r\n",
        "    clf.fit(X,Y)\r\n",
        "\r\n",
        "    # now store model to use it later in other parts\r\n",
        "    with open(documents_path + 'GNB_model.pickle', 'wb') as model_file:\r\n",
        "      pickle.dump(clf, model_file, protocol= pickle.HIGHEST_PROTOCOL)\r\n",
        "    \r\n",
        "    print('saved model')\r\n",
        "    return clf\r\n",
        "    \r\n",
        "  def evaluate(self, evaluation_json_path):\r\n",
        "\r\n",
        "    # a dictionary that wach key is a doc_id and it's value is doc paraghraph text\r\n",
        "    paragraph_dict = dict()\r\n",
        "    # a dictionary that each key has a list ,which are in the same order,\r\n",
        "    # including question's data and answer and question's paraghraph id in order\r\n",
        "    question_answer_dict = {'question':[],\r\n",
        "                                'answer':[],\r\n",
        "                                'paragraph_id':[]\r\n",
        "                                  }\r\n",
        "    with open(evaluation_json_path, 'r', encoding= 'utf8') as eval_file:\r\n",
        "      eval_data = json.load(eval_file)['data']\r\n",
        "\r\n",
        "      # use it for indexing paraghraphs\r\n",
        "      doc_id =0\r\n",
        "\r\n",
        "      for title_docs in eval_data:\r\n",
        "        for doc in title_docs['paragraphs']:\r\n",
        "          paragraph_dict.update({doc_id:doc['context']})\r\n",
        "          for qas in doc['qas']:\r\n",
        "            for answer in qas['answers']:\r\n",
        "              question_answer_dict['question'].append(qas['question'])\r\n",
        "              # store answer_start too, as we need the start index to find the sentence that asnwer is in\r\n",
        "              question_answer_dict['answer'].append(answer)\r\n",
        "              question_answer_dict['paragraph_id'].append(doc_id)\r\n",
        "          doc_id+=1\r\n",
        "\r\n",
        "    # a dictionary to store transformed_data\r\n",
        "    transformed_data = dict()\r\n",
        "    try:  # try to load transformed_data file\r\n",
        "      with open(documents_path + 'eval_transformed_data.pickle', 'rb') as transformed_data_file:\r\n",
        "        transformed_data = pickle.load(transformed_data_file)\r\n",
        "    except IOError:\r\n",
        "      print('*'*8,'required file does not exist, preparing them might take a while','*'*8)\r\n",
        "\r\n",
        "      # replace questions with refrmed ones\r\n",
        "      question_answer_dict['question'] = self.POS_tagger(question_answer_dict['question'])\r\n",
        "      # vectorize questions and paragraphs sentences\r\n",
        "      vectorized_data = self.sent_vectorize(paragraph_dict, question_answer_dict, train= False)\r\n",
        "\r\n",
        "      print('----------------   finished vectorizing    ----------------')\r\n",
        "      # calculate distances\r\n",
        "      cosine_similarity  = self.calculate_vector_distances(1, vectorized_data, question_answer_dict, train= False)\r\n",
        "      euclidean_distance = self.calculate_vector_distances(2, vectorized_data, question_answer_dict, train= False) \r\n",
        "      dot_product = self.calculate_vector_distances(3, vectorized_data, question_answer_dict, train= False)\r\n",
        "      print('----------------   finished distances    ----------------')\r\n",
        "      # compare roots \r\n",
        "      compared_roots_data = self.compare_roots(paragraph_dict, question_answer_dict, train= False)\r\n",
        "      print('----------------   finished roots comparision    ----------------')\r\n",
        "      # wh features\r\n",
        "      wh_feature_list = self.check_wh_presence(question_answer_dict, train= False)\r\n",
        "      print('----------------   finished wh feature    ----------------')\r\n",
        "\r\n",
        "      # extract linguistic feature\r\n",
        "      linguistic_feature = self.extract_linguistic_features( paragraph_dict, question_answer_dict, train= False)\r\n",
        "      print('----------------   finished linguisitc extraction    ----------------')\r\n",
        "\r\n",
        "      transformed_data = self.transform_data(sentence_linguistic_feature= linguistic_feature['paragraph'], \r\n",
        "                      cosine_similarity_dict = cosine_similarity, euclidean_distance_dict = euclidean_distance,\r\n",
        "                      dot_product_dict = dot_product, root_comparision_dict = compared_roots_data,\r\n",
        "                      wh_presence_list = wh_feature_list, question_linguistic_feature = linguistic_feature['question'], train= False)\r\n",
        "\r\n",
        "      print('----------------   finished trnsforming data    ----------------')\r\n",
        "      \r\n",
        "\r\n",
        "      with open(documents_path + 'eval_transformed_data.pickle', 'wb') as transformed_data_file:\r\n",
        "        pickle.dump(transformed_data, transformed_data_file, protocol= pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    labels = self.extract_label(paragraph_dict, question_answer_dict)\r\n",
        "    try: # try to load model\r\n",
        "      with open(documents_path + 'GNB_model.pickle', 'rb') as model_file:\r\n",
        "        model = pickle.load(model_file)\r\n",
        "\r\n",
        "        x_eval, y_eval = self.prepare_data_for_classifier(transformed_data, labels)\r\n",
        "        y_pred = model.predict(np.array(x_eval))\r\n",
        "\r\n",
        "        score = accuracy_score(y_eval, y_pred)\r\n",
        "        print(score)\r\n",
        "        return score\r\n",
        "    except IOError:\r\n",
        "      print('*'*8, 'model has not been trained, training process might take a while', '*'*8)\r\n",
        "      try:\r\n",
        "        train_transformed_data= dict()\r\n",
        "        with open(documents_path + 'train_transformed_data.pickle', 'rb') as transformed_data_file:\r\n",
        "          train_transformed_data = pickle.load(transformed_data_file)\r\n",
        "      except IOError:\r\n",
        "        print('*'*8, 'transformed data for training does not exist,  preparing it might take a while', '*'*8)\r\n",
        "        train_transformed_data= self.transform_data()\r\n",
        "\r\n",
        "      model = self.fit(train_transformed_data)      \r\n",
        "      x_eval, y_eval = self.prepare_data_for_classifier(transformed_data, labels)\r\n",
        "      y_pred = model.predict(np.array(x_eval))\r\n",
        "\r\n",
        "      score = accuracy_score(y_eval, y_pred)\r\n",
        "      print(score)\r\n",
        "      return score\r\n",
        "\r\n",
        "  def query(self, question, paragraph):\r\n",
        "      # convert given paragraph and question to the format that is acceptable by other funtions   \r\n",
        "      paragraph_dict = {0: paragraph}\r\n",
        "      question_answer_dict = {'question':[question],\r\n",
        "                              'paragraph_id':[0]\r\n",
        "                                }\r\n",
        "\r\n",
        "      # replace questions with refrmed ones\r\n",
        "      question_answer_dict['question'] = self.POS_tagger(question_answer_dict['question'])\r\n",
        "      # vectorize questions and paragraphs sentences\r\n",
        "      vectorized_data = self.sent_vectorize(paragraph_dict, question_answer_dict, train= False)\r\n",
        "\r\n",
        "      # calculate distances\r\n",
        "      cosine_similarity  = self.calculate_vector_distances(1, vectorized_data, question_answer_dict, train= False)\r\n",
        "      euclidean_distance = self.calculate_vector_distances(2,  vectorized_data, question_answer_dict, train= False) \r\n",
        "      dot_product = self.calculate_vector_distances(3, vectorized_data, question_answer_dict, train= False)\r\n",
        "      # compare roots \r\n",
        "      compared_roots_data = self.compare_roots(paragraph_dict, question_answer_dict, train= False)\r\n",
        "      # wh features\r\n",
        "      wh_feature_list = self.check_wh_presence(question_answer_dict, train= False)\r\n",
        "      # extract linguistic feature\r\n",
        "      linguistic_feature = self.extract_linguistic_features(paragraph_dict, question_answer_dict, train= False)\r\n",
        "\r\n",
        "      # transform data \r\n",
        "      transformed_data = self.transform_data(sentence_linguistic_feature= linguistic_feature['paragraph'], \r\n",
        "                      cosine_similarity_dict = cosine_similarity, euclidean_distance_dict = euclidean_distance,\r\n",
        "                      dot_product_dict = dot_product, root_comparision_dict = compared_roots_data,\r\n",
        "                      wh_presence_list = wh_feature_list, question_linguistic_feature = linguistic_feature['question'], train= False,)\r\n",
        "\r\n",
        "    \r\n",
        "      try: # try to load model\r\n",
        "        with open(documents_path + 'GNB_model.pickle', 'rb') as model_file:\r\n",
        "          model = pickle.load(model_file)\r\n",
        "          \r\n",
        "          # to storre given idenx to each label by model\r\n",
        "          class_dict = dict()\r\n",
        "          for i, label in enumerate(model.classes_):\r\n",
        "            class_dict.update({label: i})\r\n",
        "\r\n",
        "          paragraph_id = 0\r\n",
        "          question_index = 0\r\n",
        "          # get prob to choose the sentence that has been assigned to calss 1 most confidently\r\n",
        "          predict_prob = model.predict_proba(np.array(transformed_data[paragraph_id][question_index]))\r\n",
        "\r\n",
        "          sentence_index = -1\r\n",
        "          max_val = -1.0\r\n",
        "          # get index of sentence which has been assigned to class 1 most confidently \r\n",
        "          for index, prob in enumerate(predict_prob):\r\n",
        "            if prob[class_dict[1]] > max_val:\r\n",
        "              max_val = prob[class_dict[1]]\r\n",
        "              sentence_index = index\r\n",
        "          return self.sent_tokenize(paragraph_dict[0])[sentence_index]\r\n",
        "\r\n",
        "      except IOError:\r\n",
        "        print('*'*8, 'model has not been trained, training process might take a while', '*'*8)\r\n",
        "        try:  # try to open transformed data file\r\n",
        "          train_transformed_data= dict()\r\n",
        "          with open(documents_path + 'train_transformed_data.pickle', 'rb') as transformed_data_file:\r\n",
        "            train_transformed_data = pickle.load(transformed_data_file)\r\n",
        "        except IOError:\r\n",
        "          print('*'*8, 'transformed data for training does not exist,  preparing it might take a while','*'*8)\r\n",
        "          train_transformed_data= self.transform_data()\r\n",
        "\r\n",
        "        model = self.fit(train_transformed_data)      \r\n",
        "    \r\n",
        "        # to storre given idenx to each label by model\r\n",
        "        class_dict = dict()\r\n",
        "        for i, label in enumerate(model.classes_):\r\n",
        "          class_dict.update({label: i})\r\n",
        "\r\n",
        "        paragraph_id = 0\r\n",
        "        question_index = 0\r\n",
        "        # get prob to choose the sentence that has been assigned to calss 1 most confidently\r\n",
        "        predict_prob = model.predict_proba(np.array(transformed_data[paragraph_id][question_index]))\r\n",
        "\r\n",
        "        sentence_index = -1\r\n",
        "        max_val = -1.0\r\n",
        "        # get index of sentence which has been assigned to class 1 most confidently \r\n",
        "        for index, prob in enumerate(predict_prob):\r\n",
        "          if prob[class_dict[1]] > max_val:\r\n",
        "            max_val = prob[class_dict[1]]\r\n",
        "            sentence_index = index\r\n",
        "        return self.sent_tokenize(paragraph_dict[0])[sentence_index]\r\n",
        "\r\n",
        "\r\n",
        "# how many sentence each paraghrap includes at most extracted by checking each k \r\n",
        "# and how many data they contains \r\n",
        "k=7\r\n",
        "# construct an object from Answer_Sentence_Detector\r\n",
        "sample_object = Answer_Sentence_Detector(data_path + train_file, k)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# call sent_vectorize function\r\n",
        "vectorized_data = sample_object.sent_vectorize(sample_object.paragraph_dict,sample_object.question_answer_dict)\r\n",
        "\r\n",
        "# call calculate_vector_distances\r\n",
        "cosine_similarity = sample_object.calculate_vector_distances(1, vectorized_data, sample_object.question_answer_dict)\r\n",
        "euclidean_distnace = sample_object.calculate_vector_distances(2, vectorized_data, sample_object.question_answer_dict)\r\n",
        "dot_product = sample_object.calculate_vector_distances(3, vectorized_data, sample_object.question_answer_dict)\r\n",
        "\r\n",
        "# call compare roots function\r\n",
        "root_comparision = sample_object.compare_roots(samle_object.paragraph_dict, sample_object.question_answer_dict)\r\n",
        "\r\n",
        "# call check_wh_presence function\r\n",
        "wh_precense_list = sample_object.check_wh_presence(sample_object.paragraph_dict)\r\n",
        "\r\n",
        "# call extract_linguistic_feature\r\n",
        "linguistic_feature = sample_object.extract_linguistic_features(sample_object.paragraph_dict, sample_object.question_answer_dict)\r\n",
        "\r\n",
        "# call transform_data function\r\n",
        "transformed_data = sample_object.transform_data()\r\n",
        "\r\n",
        "# call fit function\r\n",
        "model = sample_object.fit(transformed_data)\r\n",
        "\r\n",
        "# call evaluate function\r\n",
        "accuracy = sample_object.evaluate(data_path + eval_file) \r\n",
        "\r\n",
        "# call query function\r\n",
        "paragraph = sample_object.paragraph_dict[0] \r\n",
        "question = sample_object.question_answer_dict['question'][0]\r\n",
        "answer_sentence = sample_object.query(question, paragraph)\r\n",
        "\r\n",
        "with open(documents_path + 'sample_object.pickle', 'wb') as sample_object_file:\r\n",
        "  pickle.dump(sample_object, sample_object_file, protocol= pickle.HIGHEST_PROTOCOL)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTVep7lX8iBV"
      },
      "source": [
        "### data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7nbDPt88r6"
      },
      "source": [
        "* save all NER that our training file has to be able to decide to use which of them "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqXG2JORtWtq"
      },
      "source": [
        "from stanza.server import CoreNLPClient\r\n",
        "ner_dict=dict()\r\n",
        "with CoreNLPClient(\r\n",
        "        annotators=['ner'],\r\n",
        "        timeout=30000,\r\n",
        "        memory='4G', endpoint='http://localhost:9001', be_quiet=True) as client:\r\n",
        "        \r\n",
        "        for i,doc in enumerate(sample.paragraph_dict.values()):\r\n",
        "          sen_ls = sample.sent_tokenize(doc)\r\n",
        "          for sentence in sen_ls:\r\n",
        "            nn= client.annotate(sentence)\r\n",
        "            for sen in nn.sentence:\r\n",
        "              for token in sen.token:\r\n",
        "                ner = token.ner\r\n",
        "                if ner in ner_dict:\r\n",
        "                  ner_dict[ner].append(token.value)\r\n",
        "                else:\r\n",
        "                  ner_dict.update({ner:[token.value]})\r\n",
        "          if i % 1000 == 0:\r\n",
        "            print('{:10s}:{:12d}  {:10s}  {:12d}'.format('processed',i,'over',len(sample.paragraph_dict)))\r\n",
        "\r\n",
        "\r\n",
        "with open(documents_path+'All_NER.json','w') as updated_qustions_file:\r\n",
        "  json.dump(ner_dict,updated_qustions_file)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEN7UB6n8m7J"
      },
      "source": [
        "* see which NER CoreNLPCLient has and which we've been used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5TKOT8BGPjX",
        "outputId": "fe2dc349-7ca2-459e-bee0-20b1c62b13b0"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "from stanza.server import CoreNLPClient\r\n",
        "\r\n",
        "\r\n",
        "with open(documents_path + 'All_NER.json', 'r', encoding = 'utf8') as NER_file:\r\n",
        "  ner_d = json.load(NER_file)\r\n",
        "  for n in ner_d.keys():\r\n",
        "      if n in linguistic_features:\r\n",
        "        print('{:10s}  {}'.format(n,True))\r\n",
        "      else:\r\n",
        "        print('{:10s}  {}'.format(n,False))\r\n",
        "{'RELIGION': 2, 'PERSON': 2, 'CITY': 0, 'COUNTRY': 0, 'TITLE': 2, 'DATE': 1, 'LOCATION': 0,  'ORGANIZATION': 0, 'IDEOLOGY': 2, 'NATIONALITY': 2, 'STATE_OR_PROVINCE': 2, 'CAUSE_OF_DEATH': 2, 'TIME': 1, 'CRIMINAL_CHARGE': 2}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O           False\n",
            "RELIGION    True\n",
            "PERSON      True\n",
            "MISC        False\n",
            "CITY        True\n",
            "COUNTRY     True\n",
            "TITLE       True\n",
            "DATE        True\n",
            "NUMBER      False\n",
            "LOCATION    True\n",
            "ORGANIZATION  True\n",
            "SET         False\n",
            "DURATION    False\n",
            "IDEOLOGY    True\n",
            "NATIONALITY  True\n",
            "ORDINAL     False\n",
            "STATE_OR_PROVINCE  True\n",
            "PERCENT     False\n",
            "URL         False\n",
            "MONEY       False\n",
            "CAUSE_OF_DEATH  True\n",
            "TIME        True\n",
            "CRIMINAL_CHARGE  True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CAUSE_OF_DEATH': 2,\n",
              " 'CITY': 0,\n",
              " 'COUNTRY': 0,\n",
              " 'CRIMINAL_CHARGE': 2,\n",
              " 'DATE': 1,\n",
              " 'IDEOLOGY': 2,\n",
              " 'LOCATION': 0,\n",
              " 'NATIONALITY': 2,\n",
              " 'ORGANIZATION': 0,\n",
              " 'PERSON': 2,\n",
              " 'RELIGION': 2,\n",
              " 'STATE_OR_PROVINCE': 2,\n",
              " 'TIME': 1,\n",
              " 'TITLE': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}